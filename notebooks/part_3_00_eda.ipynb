{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8bb3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521c4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "\n",
    "profile = ProfileReport(df, title=\"Profiling Report\", explorative=True)\n",
    "profile.to_file(\"../docs/profiling_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabe168",
   "metadata": {},
   "source": [
    "# Profiling Report Interpretation\n",
    "\n",
    "The full report can be found here: [`../docs/profiling_report.html`](../docs/profiling_report.html)\n",
    "\n",
    "## Summary Statistics\n",
    "\n",
    "From the summary statistics, we can see:\n",
    "- The training dataset has 31 variables (30 features + 1 target) and 7003 observations. The number of observations is much greater than the number of features, therefore no high-dimentional data issue here.\n",
    "- Missing values are present, which need to be explored further.\n",
    "- There are no duplicated values.\n",
    "- Training data size is only 4.8 MB, which shouldn't cause any memory issues.\n",
    "\n",
    "<img src=\"../docs/images/summary_statistics.png\" alt=\"summary statistics\" width=\"300\" />\n",
    "\n",
    "## Target Distribution\n",
    "\n",
    "The target variable has 2 values, `0` and `1`, which points to a binary classification problem. The dataset is also imblanced, with class `1` taking up ~14% of total observations. I assume the business context here is similar to a fraud detection, meaning the minority class is what we care about, so we need to use resampling methods to combat the imbalanced dataset issue. As we don't have a large number of observations, I choose to oversample instead of undersample. Common oversampling techniques with good performance include `SMOTE` and `ADASYN`.\n",
    "\n",
    "<img src=\"../docs/images/target.png\" alt=\"target\" width=\"600\" />\n",
    "\n",
    "## Missing Values\n",
    "\n",
    "From the missing values chart, we can see `Vicuna` is 100% missing, so we can drop it from the training dataset. It is a good idea to set up monitoring for Vicuna's missing percentage in case it changes in the future.\n",
    "\n",
    "<img src=\"../docs/images/missing_values.png\" alt=\"missing values\" width=\"600\" />\n",
    "\n",
    "`Tiglon` and `Wallaby` are both missing for ~50% with no particular pattern.\n",
    "\n",
    "`Tiglon` has one constant value `False`. Since we don't know if the missing value represents `True`, we will fill them with `unknown`.\n",
    "\n",
    "<img src=\"../docs/images/tiglon.png\" alt=\"tiglon\" width=\"600\" />\n",
    "\n",
    "`Wallaby` is a regular numerical feature, we will impute the missing values with an imputer, such as `KNNImputer` or `IterativeImputer`.\n",
    "\n",
    "<img src=\"../docs/images/wallaby.png\" alt=\"wallaby\" width=\"600\" />\n",
    "\n",
    "## Features\n",
    "\n",
    "Out of the 30 features, 19 of them are numerical, 10 are categorical, 1 boolean (`Tiglon`, will be converted to categorical after filling missing values) and 1 unsupported consisting of all null values (`Vicuna`, will be dropped).\n",
    "\n",
    "<img src=\"../docs/images/variable_types.png\" alt=\"variable types\" width=\"300\" />\n",
    "\n",
    "### Numerical Features\n",
    "\n",
    "Upon closer inspection, I noticed two numerical features, `Thrush` and `Turtle`, naturally falling into a few bins from the histograms. Assuming this behaviour fits the business context, converting them into categorical variables by applying K-bins discretization will reduce variance from these features.\n",
    "\n",
    "<img src=\"../docs/images/thrush.png\" alt=\"thrush\" width=\"600\" />\n",
    "<img src=\"../docs/images/turtle.png\" alt=\"turtle\" width=\"600\" />\n",
    "\n",
    "The rest of the numerical features have wildly different scales, so I decided to standardize them to avoid any potential impact from scales.\n",
    "\n",
    "I noticed there might be outliers present, e.g. in features `Viper`, `Turkey`, but due to the lack of business context to determine if these values are valid or invalid, I decided to keep them to be safe.\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "`Vulture` and `Warbler` have high cardinality issues with distinct categories of 40 and 83, respectively. This can be resolved by using the hashing trick.\n",
    "<img src=\"../docs/images/vulture.png\" alt=\"vulture\" width=\"600\" />\n",
    "<img src=\"../docs/images/warbler.png\" alt=\"warbler\" width=\"600\" />\n",
    "\n",
    "`Tiger`, `Toad`, `Wildfowl`, `Wolf`, and `Wolverine` have the rare-category problem, meaning they have categories representing only a tiny percentage of the total population. Leaving them untreated will result in a sparse matrix after one-hot encoding and therefore affect model performance. I decided to re-group any categories with less than 5% representation into a \"rare\" category.\n",
    "\n",
    "Showing `Toad` as an example:\n",
    "<img src=\"../docs/images/toad.png\" alt=\"toad\" width=\"600\" />\n",
    "\n",
    "\n",
    "## Correlation\n",
    "\n",
    "There are high correlations between some features. I decided to not perform PCA or drop any correlated features, since 1) we don't have a high dimensionality problem and 2) the goal here is prediction rather than interpretation. As long as we steer away from algorithms that get impacted by multicollinearity, such as Logistic Regression or Naive Bayes, the additional features won't become a problem and could add to the predictive power.\n",
    "\n",
    "<img src=\"../docs/images/correlation.png\" alt=\"correlation\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fb555",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In summary, here are the preprocessing steps to perform before modelling:\n",
    "\n",
    "## Feature Preprocessing\n",
    "\n",
    "1. Drop `Vicuna`\n",
    "3. Convert `Tiglon` from boolean to categorical feature by filling missing values with string `unknown`\n",
    "2. Convert `Thrush` and `Turtle` to categorical features using K-bins discretization\n",
    "\n",
    "### Numerical Features\n",
    "\n",
    "4. Impute `Wallaby`'s missing values with an imputer, e.g. `KNNImputer` or `IterativeImputer`\n",
    "5. Standardize all numerical features\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "6. Encode high-cardinal features using the hashing trick for `Vulture` and `Warbler`\n",
    "7. Re-assign minority categories to `rare` for `Tiger`, `Toad`, `Wildfowl`, `Wolf`, and `Wolverine`\n",
    "8. One-hot encode low-cardinal features\n",
    "\n",
    "## Target Preprocessing\n",
    "\n",
    "1. Resample `target`'s minority class (`1`) to fix imbalanced dataset issue using `SMOTE` or `ADASYN`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
